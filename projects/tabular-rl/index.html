<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tabular Reinforcement Learning | Minsuan Teh </title> <meta name="author" content="Minsuan Teh"> <meta name="description" content="Solving Taxi-v3 using Q-Learning and On-Policy First Visit Monte Carlo"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minsuan96.github.io/projects/tabular-rl/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Minsuan</span> Teh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/experiences/">experiences </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tabular Reinforcement Learning</h1> <p class="post-description">Solving Taxi-v3 using Q-Learning and On-Policy First Visit Monte Carlo</p> </header> <article> <p>_Disclaimer: The code for this project is not accessible to the public as it constitutes one of the assignments for the <a href="https://opencourse.inf.ed.ac.uk/rl" rel="external nofollow noopener" target="_blank">Reinforcement Learning</a> course at <a href="https://www.ed.ac.uk/" rel="external nofollow noopener" target="_blank">The University of Edinburgh</a>.*</p> <ul> <li><a href="#taxi-v3">Taxi-v3</a></li> <li> <a href="#setting-up-the-agent">Setting up the Agent</a> <ul> <li><a href="#q-learning-agent">Q-Learning Agent</a></li> <li><a href="#monte-carlo-agent">Monte Carlo Agent</a></li> </ul> </li> <li> <a href="#training-the-agent">Training the Agent</a> <ul> <li><a href="#using-q-learning">Using Q-Learning</a></li> <li><a href="#using-on-policy-first-visit-monte-carlo">Using On-Policy First Visit Monte Carlo</a></li> </ul> </li> <li> <a href="#results">Results</a> <ul> <li> <a href="#q-learning">Q-Learning</a> <ul> <li><a href="#observations">Observations</a></li> <li><a href="#quantifying-performance">Quantifying Performance</a></li> <li><a href="#insights">Insights</a></li> </ul> </li> <li> <a href="#monte-carlo">Monte Carlo</a> <ul> <li><a href="#observations-1">Observations</a></li> <li><a href="#quantifying-performance-1">Quantifying Performance</a></li> <li><a href="#insights-1">Insights</a></li> </ul> </li> </ul> </li> </ul> <h1 id="taxi-v3">Taxi-v3</h1> <div class="row justify-content-center"> <div class="col-md-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-480.webp 480w,/assets/img/taxi-800.webp 800w,/assets/img/taxi-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://gymnasium.farama.org/environments/toy_text/taxi/" rel="external nofollow noopener" target="_blank">The Taxi environment</a>, part of the Toy Text environments, simulates a scenario where a taxi navigates a grid world to pick up and drop off passengers at designated locations. The grid world is a 5x5 grid with four designated pick-up and drop-off locations marked by colors: Red, Green, Yellow, and Blue.</p> <p>At the start of each episode, the taxi is placed randomly within the grid, and a passenger is positioned at one of the designated locations. The objective is for the taxi to navigate to the passenger’s location, pick up the passenger, move to the passenger’s desired destination, and then drop off the passenger. Once the passenger is successfully dropped off, the episode concludes.</p> <p>The action space is discrete with six possible actions:</p> <ul> <li>0: Move south (down)</li> <li>1: Move north (up)</li> <li>2: Move east (right)</li> <li>3: Move west (left)</li> <li>4: Pickup passenger</li> <li>5: Drop off passenger</li> </ul> <p>The observation space is also discrete, with 500 possible states representing different combinations of taxi position, passenger location, and destination.</p> <p>Each step in the environment incurs a penalty of -1 unless a specific action triggers a different reward. Successfully dropping off a passenger yields a reward of +20. However, executing illegal actions such as attempting to pick up or drop off a passenger at an incorrect location incurs a penalty of -10.</p> <p>Episodes can end in two ways:</p> <ol> <li>The passenger is successfully dropped off at their destination.</li> <li>If a time limit is set, the episode ends after a fixed number of steps (usually 200).</li> </ol> <hr> <h1 id="setting-up-the-agent">Setting up the Agent</h1> <p>An abstract class, <code class="language-plaintext highlighter-rouge">Agent</code>, for the agent has been implemented as a base class for both the Q-learning agent and the Monte Carlo agent. It consists of a total of four functions, two of which are abstract.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Space</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">Space</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">schedule_hyperparameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__init__</code> function initializes the basic variables of the class, including:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">action_space</code>: action space of the environment</li> <li> <code class="language-plaintext highlighter-rouge">obs_space</code>: observation space of the environment</li> <li> <code class="language-plaintext highlighter-rouge">gamma</code>: discount factor</li> <li> <code class="language-plaintext highlighter-rouge">epsilon</code>: epsilon for epsilon-greedy action selection</li> <li> <code class="language-plaintext highlighter-rouge">n_acts</code>: number of actions</li> <li> <code class="language-plaintext highlighter-rouge">q_table</code>: table for Q-values mapping pairs of observations and actions to respective Q-values</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">act</code> function takes an observation as input and uses epsilon-greedy selection to return the index of the selected action. Before each episode, the <code class="language-plaintext highlighter-rouge">schedule_hyperparameters</code> function is called to adjust epsilon. It takes current timestep at the beginning of the episode and the maximum timesteps that the training loop will run for as arguments. The <code class="language-plaintext highlighter-rouge">learn</code> function updates the Q-table based on the agent’s experience.</p> <h2 id="q-learning-agent">Q-Learning Agent</h2> <p>A class, <code class="language-plaintext highlighter-rouge">QLearningAgent</code>, that inherits from <code class="language-plaintext highlighter-rouge">Agent</code> has been implemented as the Q-Learning agent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">QLearningAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">schedule_hyperparameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__init__</code> function initializes an additional variable called <code class="language-plaintext highlighter-rouge">alpha</code> which represents the learning rate of the agent. The <code class="language-plaintext highlighter-rouge">schedule_hyperparameters</code> function uses linear decay to adjust epsilon, using a different gradient and minumum compared to those used in the Monte Carlo agent. The <code class="language-plaintext highlighter-rouge">learn</code> function takes several arguments as input, including:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">obs</code>: received observation representing the current environmental state</li> <li> <code class="language-plaintext highlighter-rouge">action</code>: index of applied action</li> <li> <code class="language-plaintext highlighter-rouge">reward</code>: received reward</li> <li> <code class="language-plaintext highlighter-rouge">n_obs</code>: received observation representing the next environmental state</li> <li> <code class="language-plaintext highlighter-rouge">done</code>: flag indicating whether a terminal state has been reached</li> </ul> <p>Then, it implements the Q-Learning algorithm to return the updated Q-value for current observation-action pair.</p> <h2 id="monte-carlo-agent">Monte Carlo Agent</h2> <p>A class, <code class="language-plaintext highlighter-rouge">MonteCarloAgent</code>, that inherits from <code class="language-plaintext highlighter-rouge">Agent</code> has been implemented as the Monte Carlo agent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MonteCarloAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">schedule_hyperparameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">obses</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">actions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">rewards</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__init__</code> function initializes an additional variable called <code class="language-plaintext highlighter-rouge">sa_counts</code> which is a dicionary used to count occurrences observation-action pairs. The <code class="language-plaintext highlighter-rouge">schedule_hyperparameters</code> function uses linear decay to adjust epsilon, using a different gradient and minumum compared to those used in the Q-Learning agent. The <code class="language-plaintext highlighter-rouge">learn</code> function takes several arguments as input, including:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">obses</code>: list of received observations representing environmental states of trajectory (in the order they were encountered)</li> <li> <code class="language-plaintext highlighter-rouge">actions</code>: list of indices of applied actions in trajectory (in the order they were applied)</li> <li> <code class="language-plaintext highlighter-rouge">rewards</code>: list of received rewards during trajectory (in the order they were received)</li> </ul> <p>Then, it implements the On-Policy First Visit Monte Carlo algorithm to return a dictionary containing the updated Q-value of all the updated state-action pairs indexed by the state action pair.</p> <hr> <h1 id="training-the-agent">Training the Agent</h1> <p>The environment that the agents train and evaluate on are set with the following parameters:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">env</code>: “Taxi-v3”</li> <li> <code class="language-plaintext highlighter-rouge">eps_max_steps</code>: 50</li> <li> <code class="language-plaintext highlighter-rouge">eval_episodes</code>: 500</li> <li> <code class="language-plaintext highlighter-rouge">eval_eps_max_steps</code>: 100,</li> <li> <code class="language-plaintext highlighter-rouge">total_eps</code> of Monte Carlo agent: 100000</li> <li> <code class="language-plaintext highlighter-rouge">total_eps</code> of Q-Learning agent: 10000</li> </ul> <p>Both agents use the same <code class="language-plaintext highlighter-rouge">evaluate</code> function, which is implemented in <code class="language-plaintext highlighter-rouge">utils.py</code> for the evaluation of agent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">eval_episodes</span><span class="p">,</span> <span class="n">render</span><span class="p">)</span>
</code></pre></div></div> <p>The function accepts 5 parameters as its arguments, which are:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">env</code>: environment to execute evaluation on</li> <li> <code class="language-plaintext highlighter-rouge">agent</code>: agent to act in environment</li> <li> <code class="language-plaintext highlighter-rouge">max_steps</code>: max number of steps per evaluation episode</li> <li> <code class="language-plaintext highlighter-rouge">eval_episodes</code>: number of evaluation episodes</li> <li> <code class="language-plaintext highlighter-rouge">render</code>: flag whether evaluation runs should be rendered</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">evaluate</code> function assesses the given configuration on the provided environment, using the specified agent, and returns two pieces of information:</p> <ul> <li>The mean return received over all evaluation episodes.</li> <li>The number of evaluation episodes where the return was negative.</li> </ul> <p>This function serves to evaluate the performance of the agent in the given environment and provides insights into its effectiveness in completing tasks and avoiding negative outcomes.</p> <h2 id="using-q-learning">Using Q-Learning</h2> <p>Training the agent with Q-Learning is done in <code class="language-plaintext highlighter-rouge">train_q_learning.py</code>. The Python file consists of two functions, <code class="language-plaintext highlighter-rouge">q_learning_eval</code> and <code class="language-plaintext highlighter-rouge">train</code>, and a few configurable constants, <code class="language-plaintext highlighter-rouge">eval_freq</code>, <code class="language-plaintext highlighter-rouge">alpha</code>, <code class="language-plaintext highlighter-rouge">epsilon</code> and <code class="language-plaintext highlighter-rouge">gamma</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">eval_freq</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">epsilon</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">def</span> <span class="nf">q_learning_eval</span><span class="p">(</span><span class="n">env</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">q_table</span><span class="p">,</span>
        <span class="n">render</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">q_learning_eval</code> accepts 5 parameters as its arguments which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> <li>q_table: Q-table mapping observation-action to Q-values</li> <li>render: flag whether evaluation runs should be rendered</li> <li>output: flag whether mean evaluation performance should be printed</li> </ul> <p>This function uses <code class="language-plaintext highlighter-rouge">utils.evaluate</code> to evaluate the configuration of Q-learning on given environment when initialised with given Q-table. Then, it returns the mean and standard deviation of returns received over episodes.</p> <p>On the other hand, <code class="language-plaintext highlighter-rouge">train</code> accepts 3 parameters as its arguments, which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> <li>output: flag if mean evaluation results should be printed</li> </ul> <p>It trains the Q-Learning agent and calls <code class="language-plaintext highlighter-rouge">q_learning_eval</code> to evaluate on given environment with provided hyperparameters. Then, it returns the total reward over all episodes, list of means and standard deviations of evaluation returns and the final Q-table.</p> <h2 id="using-on-policy-first-visit-monte-carlo">Using On-Policy First Visit Monte Carlo</h2> <p>Training the agent with Monte Carlo is done in <code class="language-plaintext highlighter-rouge">train_monte_carlo.py</code>. The Python file consists of two functions, <code class="language-plaintext highlighter-rouge">monte_carlo_eval</code> and <code class="language-plaintext highlighter-rouge">train</code>, and a few configurable constants, <code class="language-plaintext highlighter-rouge">eval_freq</code>, <code class="language-plaintext highlighter-rouge">epsilon</code> and <code class="language-plaintext highlighter-rouge">gamma</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">eval_freq</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">epsilon</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">def</span> <span class="nf">monte_carlo_eval</span><span class="p">(</span><span class="n">env</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">q_table</span><span class="p">,</span>
        <span class="n">render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">monte_carlo_eval</code> accepts 4 parameters as its arguments which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> <li>q_table: Q-table mapping observation-action to Q-values</li> <li>render: flag whether evaluation runs should be rendered</li> </ul> <p>This function uses <code class="language-plaintext highlighter-rouge">utils.evaluate</code> to evaluate the configuration of Monte Carlo on given environment when initialised with given Q-table. Then, it returns the mean and standard deviation of returns received over episodes.</p> <p>On the other hand, <code class="language-plaintext highlighter-rouge">train</code> accepts 2 parameters as its arguments, which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> </ul> <p>It trains the Monte Carlo agent and calls <code class="language-plaintext highlighter-rouge">monte_carlo_eval</code> to evaluate on given environment with provided hyperparameters. Then, it returns the total reward over all episodes, list of means and standard deviations of evaluation returns and the final Q-table.</p> <hr> <h1 id="results">Results</h1> <h2 id="q-learning">Q-Learning</h2> <div class="container"> <div class="row justify-content-center"> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-q1000-480.webp 480w,/assets/img/taxi-q1000-800.webp 800w,/assets/img/taxi-q1000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-q1000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-q2000-480.webp 480w,/assets/img/taxi-q2000-800.webp 800w,/assets/img/taxi-q2000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-q2000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-q10000-480.webp 480w,/assets/img/taxi-q10000-800.webp 800w,/assets/img/taxi-q10000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-q10000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Q-Learning Agent trained with 1000, 2000 and 10000 episodes. </div> </div> <p>Let’s analyze the performance of the agents, each trained with a varying number of episodes (1000, 2000, and 10000) and evaluated over a number of episodes based on <code class="language-plaintext highlighter-rouge">eval_episodes</code>. We’ll use a consistent naming convention to refer to these agents, such as q1000, q2000, and q10000.</p> <h3 id="observations">Observations</h3> <ol> <li>q1000: Struggled to locate and transport the passenger efficiently。</li> <li>q2000: Demonstrated significant improvement, efficiently navigating to the passenger and completing the task.</li> <li>q10000: Exhibited comparable performance to q2000, as seen in the GIFs.</li> </ol> <h3 id="quantifying-performance">Quantifying Performance</h3> <p>To quantify their effectiveness, we calculate the mean returns from evaluation episodes with the total number of episodes equals to <code class="language-plaintext highlighter-rouge">eval_episodes</code>. Remarkably, the mean return of q2000 closely resembles that of q10000, despite fewer training iterations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">1000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="o">-</span><span class="mf">43.402</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">2000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.486</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">3000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.814</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">4000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.734</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">5000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.832</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">6000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.892</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">7000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.814</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">8000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">8.028</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">9000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">8.03</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">10000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.896</span>
</code></pre></div></div> <h3 id="insights">Insights</h3> <p>Increasing the number of training episodes generally leads to enhanced performance. However, the policy may hardly improve further. In this case, q2000 achieved similar effectiveness to q10000 despite fewer training iterations.</p> <h2 id="monte-carlo">Monte Carlo</h2> <div class="container"> <div class="row justify-content-center"> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-mc10000-480.webp 480w,/assets/img/taxi-mc10000-800.webp 800w,/assets/img/taxi-mc10000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-mc10000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-mc60000-480.webp 480w,/assets/img/taxi-mc60000-800.webp 800w,/assets/img/taxi-mc60000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-mc60000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-mc100000-480.webp 480w,/assets/img/taxi-mc100000-800.webp 800w,/assets/img/taxi-mc100000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-mc100000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Monte Carlo Agent trained with 10000, 60000 and 100000 episodes. </div> </div> <p>Let’s analyze the performance of the agents, each trained with a varying number of episodes (10000, 60000, and 100000) and evaluated over a number of episodes based on <code class="language-plaintext highlighter-rouge">eval_episodes</code>. We’ll use a consistent naming convention to refer to these agents, such as mc10000, mc60000, and mc100000.</p> <h3 id="observations-1">Observations</h3> <ol> <li>mc10000: Struggled to efficiently locate and transport the passenger, similar to q1000.</li> <li>mc60000: Demonstrated significant improvement, efficiently navigating to the passenger and completing the task.</li> <li>mc100000: Exhibited comparable performance to mc60000, as seen in the GIFs.</li> </ol> <h3 id="quantifying-performance-1">Quantifying Performance</h3> <p>To quantify their effectiveness, we calculate the mean returns from evaluation episodes with the total number of episodes equals to <code class="language-plaintext highlighter-rouge">eval_episodes</code>. Remarkably, the mean return of mc60000 closely resembles that of mc100000, despite fewer training iterations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">10000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="o">-</span><span class="mf">90.462</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">20000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="o">-</span><span class="mf">77.412</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">30000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="o">-</span><span class="mf">67.644</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">40000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="o">-</span><span class="mf">17.718</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">50000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="o">-</span><span class="mf">5.392</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">60000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.536</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">70000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.402</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">80000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.502</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">90000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">7.798</span>
<span class="n">EVALUATION</span><span class="p">:</span> <span class="n">EP</span> <span class="mi">100000</span> <span class="o">-</span> <span class="n">MEAN</span> <span class="n">RETURN</span> <span class="mf">8.1</span>
</code></pre></div></div> <h3 id="insights-1">Insights</h3> <p>Monte Carlo agents required more training iterations compared to Q-Learning agents to achieve optimal performance. While Monte Carlo agents require more iterations to converge, they can achieve comparable performance to Q-learning agents.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Minsuan Teh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>