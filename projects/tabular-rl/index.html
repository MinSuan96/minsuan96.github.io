<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tabular Reinforcement Learning | Minsuan Teh </title> <meta name="author" content="Minsuan Teh"> <meta name="description" content="Solving Taxi-v3 using Q-Learning and On-Policy First Visit Monte Carlo"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minsuan96.github.io/projects/tabular-rl/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Minsuan</span> Teh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/experiences/">experiences </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tabular Reinforcement Learning</h1> <p class="post-description">Solving Taxi-v3 using Q-Learning and On-Policy First Visit Monte Carlo</p> </header> <article> <p><em>Disclaimer: The code for this project is not accessible to the public as it constitutes one of the assignments for the <a href="https://opencourse.inf.ed.ac.uk/rl" rel="external nofollow noopener" target="_blank">Reinforcement Learning</a> course at <a href="https://www.ed.ac.uk/" rel="external nofollow noopener" target="_blank">The University of Edinburgh</a>.</em></p> <ul> <li><a href="#taxi-v3">Taxi-v3</a></li> <li> <a href="#setting-up-the-agent">Setting up the Agent</a> <ul> <li><a href="#q-learning-agent">Q-Learning Agent</a></li> <li><a href="#monte-carlo-agent">Monte Carlo Agent</a></li> </ul> </li> <li> <a href="#training-the-agent">Training the Agent</a> <ul> <li><a href="#using-q-learning">Using Q-Learning</a></li> <li><a href="#using-on-policy-first-visit-monte-carlo">Using On-Policy First Visit Monte Carlo</a></li> </ul> </li> <li> <a href="#results">Results</a> <ul> <li><a href="#q-learning">Q-Learning</a></li> <li><a href="#monte-carlo">Monte Carlo</a></li> </ul> </li> </ul> <h1 id="taxi-v3">Taxi-v3</h1> <div class="row justify-content-center"> <div class="col-md-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-480.webp 480w,/assets/img/taxi-800.webp 800w,/assets/img/taxi-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The presented environment is part of the Toy Text environments in <a href="https://gymnasium.farama.org/" rel="external nofollow noopener" target="_blank">Gymnasium</a>, specifically the <a href="https://gymnasium.farama.org/environments/toy_text/taxi/" rel="external nofollow noopener" target="_blank">Taxi-v3</a> environment. In this 5x5 grid world, the taxi must navigate to passengers at four designated locations (Red, Green, Yellow, and Blue), pick them up, and drop them off at their desired destinations. The action space is discrete with six possible actions, including movement in different directions, picking up, and dropping off passengers. There are 500 discrete states, considering taxi positions, passenger locations, and destination locations. The episode starts randomly, and rewards are given for successful passenger drop-offs, with penalties for incorrect pickup/drop-off actions. The episode ends when the passenger is dropped off or after 200 steps. This project uses Q-Learning and On-Policy First Visit Monte Carlo to solve the given environment.</p> <hr> <h1 id="setting-up-the-agent">Setting up the Agent</h1> <p>An abstract class, <code class="language-plaintext highlighter-rouge">Agent</code>, for the agent has been implemented as a base class for both the Q-learning agent and the Monte Carlo agent. It consists of a total of four functions, two of which are abstract.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Space</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">Space</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">schedule_hyperparameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__init__</code> function initializes the basic variables of the class, including:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">action_space</code>: action space of the environment</li> <li> <code class="language-plaintext highlighter-rouge">obs_space</code>: observation space of the environment</li> <li> <code class="language-plaintext highlighter-rouge">gamma</code>: discount factor</li> <li> <code class="language-plaintext highlighter-rouge">epsilon</code>: epsilon for epsilon-greedy action selection</li> <li> <code class="language-plaintext highlighter-rouge">n_acts</code>: number of actions</li> <li> <code class="language-plaintext highlighter-rouge">q_table</code>: table for Q-values mapping pairs of observations and actions to respective Q-values</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">act</code> function takes an observation as input and uses epsilon-greedy selection to return the index of the selected action. Before each episode, the <code class="language-plaintext highlighter-rouge">schedule_hyperparameters</code> function is called to adjust epsilon. It takes current timestep at the beginning of the episode and the maximum timesteps that the training loop will run for as arguments. The <code class="language-plaintext highlighter-rouge">learn</code> function updates the Q-table based on the agent’s experience.</p> <h2 id="q-learning-agent">Q-Learning Agent</h2> <p>A class, <code class="language-plaintext highlighter-rouge">QLearningAgent</code>, that inherits from <code class="language-plaintext highlighter-rouge">Agent</code> has been implemented as the Q-Learning agent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">QLearningAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">schedule_hyperparameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__init__</code> function initializes an additional variable called <code class="language-plaintext highlighter-rouge">alpha</code> which represents the learning rate of the agent. The <code class="language-plaintext highlighter-rouge">schedule_hyperparameters</code> function uses linear decay to adjust epsilon, using a different gradient and minumum compared to those used in the Monte Carlo agent. The <code class="language-plaintext highlighter-rouge">learn</code> function takes several arguments as input, including:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">obs</code>: received observation representing the current environmental state</li> <li> <code class="language-plaintext highlighter-rouge">action</code>: index of applied action</li> <li> <code class="language-plaintext highlighter-rouge">reward</code>: received reward</li> <li> <code class="language-plaintext highlighter-rouge">n_obs</code>: received observation representing the next environmental state</li> <li> <code class="language-plaintext highlighter-rouge">done</code>: flag indicating whether a terminal state has been reached</li> </ul> <p>Then, it implements the Q-Learning algorithm to return the updated Q-value for current observation-action pair.</p> <h2 id="monte-carlo-agent">Monte Carlo Agent</h2> <p>A class, <code class="language-plaintext highlighter-rouge">MonteCarloAgent</code>, that inherits from <code class="language-plaintext highlighter-rouge">Agent</code> has been implemented as the Monte Carlo agent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MonteCarloAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">schedule_hyperparameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">obses</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">actions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">rewards</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__init__</code> function initializes an additional variable called <code class="language-plaintext highlighter-rouge">sa_counts</code> which is a dicionary used to count occurrences observation-action pairs. The <code class="language-plaintext highlighter-rouge">schedule_hyperparameters</code> function uses linear decay to adjust epsilon, using a different gradient and minumum compared to those used in the Q-Learning agent. The <code class="language-plaintext highlighter-rouge">learn</code> function takes several arguments as input, including:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">obses</code>: list of received observations representing environmental states of trajectory (in the order they were encountered)</li> <li> <code class="language-plaintext highlighter-rouge">actions</code>: list of indices of applied actions in trajectory (in the order they were applied)</li> <li> <code class="language-plaintext highlighter-rouge">rewards</code>: list of received rewards during trajectory (in the order they were received)</li> </ul> <p>Then, it implements the On-Policy First Visit Monte Carlo algorithm to return a dictionary containing the updated Q-value of all the updated state-action pairs indexed by the state action pair.</p> <hr> <h1 id="training-the-agent">Training the Agent</h1> <p>The environment that the agents train and evaluate on are set with the following parameters:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">env</code>: “Taxi-v3”</li> <li> <code class="language-plaintext highlighter-rouge">eps_max_steps</code>: 50</li> <li> <code class="language-plaintext highlighter-rouge">eval_episodes</code>: 500</li> <li> <code class="language-plaintext highlighter-rouge">eval_eps_max_steps</code>: 100,</li> <li> <code class="language-plaintext highlighter-rouge">total_eps</code> of Monte Carlo agent: 100000</li> <li> <code class="language-plaintext highlighter-rouge">total_eps</code> of Q-Learning agent: 10000</li> </ul> <p>Both agents use the same <code class="language-plaintext highlighter-rouge">evaluate</code> function, which is implemented in <code class="language-plaintext highlighter-rouge">utils.py</code> for the evaluation of agent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">eval_episodes</span><span class="p">,</span> <span class="n">render</span><span class="p">)</span>
</code></pre></div></div> <p>The function accepts 5 parameters as its arguments, which are:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">env</code>: environment to execute evaluation on</li> <li> <code class="language-plaintext highlighter-rouge">agent</code>: agent to act in environment</li> <li> <code class="language-plaintext highlighter-rouge">max_steps</code>: max number of steps per evaluation episode</li> <li> <code class="language-plaintext highlighter-rouge">eval_episodes</code>: number of evaluation episodes</li> <li> <code class="language-plaintext highlighter-rouge">render</code>: flag whether evaluation runs should be rendered</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">evaluate</code> function assesses the given configuration on the provided environment, using the specified agent, and returns two pieces of information:</p> <ul> <li>The mean return received over all evaluation episodes.</li> <li>The number of evaluation episodes where the return was negative.</li> </ul> <p>This function serves to evaluate the performance of the agent in the given environment and provides insights into its effectiveness in completing tasks and avoiding negative outcomes.</p> <h2 id="using-q-learning">Using Q-Learning</h2> <p>Training the agent with Q-Learning is done in <code class="language-plaintext highlighter-rouge">train_q_learning.py</code>. The Python file consists of two functions, <code class="language-plaintext highlighter-rouge">q_learning_eval</code> and <code class="language-plaintext highlighter-rouge">train</code>, and a few configurable constants, <code class="language-plaintext highlighter-rouge">eval_freq</code>, <code class="language-plaintext highlighter-rouge">alpha</code>, <code class="language-plaintext highlighter-rouge">epsilon</code> and <code class="language-plaintext highlighter-rouge">gamma</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">eval_freq</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">epsilon</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">def</span> <span class="nf">q_learning_eval</span><span class="p">(</span><span class="n">env</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">q_table</span><span class="p">,</span>
        <span class="n">render</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">q_learning_eval</code> accepts 5 parameters as its arguments which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> <li>q_table: Q-table mapping observation-action to Q-values</li> <li>render: flag whether evaluation runs should be rendered</li> <li>output: flag whether mean evaluation performance should be printed</li> </ul> <p>This function uses <code class="language-plaintext highlighter-rouge">utils.evaluate</code> to evaluate the configuration of Q-learning on given environment when initialised with given Q-table. Then, it returns the mean and standard deviation of returns received over episodes.</p> <p>On the other hand, <code class="language-plaintext highlighter-rouge">train</code> accepts 3 parameters as its arguments, which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> <li>output: flag if mean evaluation results should be printed</li> </ul> <p>It trains the Q-Learning agent and calls <code class="language-plaintext highlighter-rouge">q_learning_eval</code> to evaluate on given environment with provided hyperparameters. Then, it returns the total reward over all episodes, list of means and standard deviations of evaluation returns and the final Q-table.</p> <h2 id="using-on-policy-first-visit-monte-carlo">Using On-Policy First Visit Monte Carlo</h2> <p>Training the agent with Monte Carlo is done in <code class="language-plaintext highlighter-rouge">train_monte_carlo.py</code>. The Python file consists of two functions, <code class="language-plaintext highlighter-rouge">monte_carlo_eval</code> and <code class="language-plaintext highlighter-rouge">train</code>, and a few configurable constants, <code class="language-plaintext highlighter-rouge">eval_freq</code>, <code class="language-plaintext highlighter-rouge">epsilon</code> and <code class="language-plaintext highlighter-rouge">gamma</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">eval_freq</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">epsilon</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">def</span> <span class="nf">monte_carlo_eval</span><span class="p">(</span><span class="n">env</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">q_table</span><span class="p">,</span>
        <span class="n">render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">monte_carlo_eval</code> accepts 4 parameters as its arguments which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> <li>q_table: Q-table mapping observation-action to Q-values</li> <li>render: flag whether evaluation runs should be rendered</li> </ul> <p>This function uses <code class="language-plaintext highlighter-rouge">utils.evaluate</code> to evaluate the configuration of Monte Carlo on given environment when initialised with given Q-table. Then, it returns the mean and standard deviation of returns received over episodes.</p> <p>On the other hand, <code class="language-plaintext highlighter-rouge">train</code> accepts 2 parameters as its arguments, which are:</p> <ul> <li>env: environment to execute evaluation on</li> <li>config: configuration dictionary containing hyperparameters</li> </ul> <p>It trains the Monte Carlo agent and calls <code class="language-plaintext highlighter-rouge">monte_carlo_eval</code> to evaluate on given environment with provided hyperparameters. Then, it returns the total reward over all episodes, list of means and standard deviations of evaluation returns and the final Q-table.</p> <hr> <h1 id="results">Results</h1> <h2 id="q-learning">Q-Learning</h2> <div class="container"> <div class="row justify-content-center"> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-q1000-480.webp 480w,/assets/img/taxi-q1000-800.webp 800w,/assets/img/taxi-q1000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-q1000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-q2000-480.webp 480w,/assets/img/taxi-q2000-800.webp 800w,/assets/img/taxi-q2000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-q2000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/taxi-q10000-480.webp 480w,/assets/img/taxi-q10000-800.webp 800w,/assets/img/taxi-q10000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/taxi-q10000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Q-Learning Agent trained with 1000, 2000 and 10000 episodes. </div> </div> <h2 id="monte-carlo">Monte Carlo</h2> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Minsuan Teh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>